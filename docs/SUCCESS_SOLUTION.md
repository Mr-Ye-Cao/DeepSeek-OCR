# ‚úÖ SOLVED: RTX 5090 Working with DeepSeek-OCR!

## The Problem

**It was NOT vLLM** - it was the **CUDA version**!

- ‚ùå **CUDA 12.4**: Incomplete sm_120 kernel support
- ‚úÖ **CUDA 12.8**: Full sm_120 (RTX 5090) support

## The Solution

Install PyTorch nightly with **CUDA 12.8**:

```bash
conda activate deepseek-ocr
pip uninstall torch torchvision torchaudio -y
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

## What Now Works ‚úÖ

1. **RTX 5090 fully supported** - no warnings, no errors
2. **Model loading works** - DeepSeek-OCR loads successfully
3. **Inference works** - OCR extraction successful
4. **Output generated** - `.mmd` files with markdown + bounding boxes

## Test Results

**Test Image:** Simple document with text and numbers

**OCR Output:**
```markdown
## Sample Document

This is a test document for OCR It contains multiple lines of text.

1 First item 2. Second item 3. Third item

Total: $42.00
```

**Files Generated:**
- `output/result.mmd` - Markdown with detection tags
- `output/result_with_boxes.jpg` - Image with bounding boxes

**Performance Stats:**
- Valid image tokens: 64 (Tiny mode)
- Output text tokens: 113
- Compression ratio: 1.77

## Current Setup

**Environment:** `deepseek-ocr` conda environment

**Versions:**
- PyTorch: 2.10.0.dev20251021+cu128
- torchvision: 0.25.0.dev20251022+cu128
- CUDA: 12.8
- GPU: RTX 5090 (sm_120)

**Mode:** Tiny (512√ó512, 64 vision tokens)

**Note:** Running without flash-attention (using eager mode) for compatibility

## How to Use

### Activate Environment:
```bash
conda activate deepseek-ocr
```

### Run Inference:
```bash
cd /home/ye/ml-experiments/DeepSeek-OCR
python test_transformers_inference_no_flash.py
```

### Or Use the Script Directly:
```python
from transformers import AutoModel, AutoTokenizer
import torch

model_name = 'deepseek-ai/DeepSeek-OCR'
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(
    model_name,
    _attn_implementation='eager',
    trust_remote_code=True,
    use_safetensors=True
)
model = model.eval().cuda().to(torch.bfloat16)

res = model.infer(
    tokenizer,
    prompt="<image>\n<|grounding|>Convert the document to markdown.",
    image_file="your_image.jpg",
    output_path="output",
    base_size=512,
    image_size=512,
    crop_mode=False,
    save_results=True
)
```

## Next Steps

### For Better Performance:
1. **Upgrade to higher resolution modes** (edit config.py):
   - Base: 1024√ó1024 (256 tokens)
   - Gundam: Dynamic cropping (adaptive tiles)

2. **Install flash-attention** (optional, for speed):
   ```bash
   export CUDA_HOME=/home/ye/miniconda3/envs/deepseek-ocr
   pip install flash-attn==2.7.3 --no-build-isolation
   ```
   Then change `_attn_implementation='flash_attention_2'` in the script

### For vLLM (Production):
vLLM support will require waiting for official updates compatible with PyTorch 2.10+/CUDA 12.8

## Summary

**Root Cause:** PyTorch built with CUDA 12.4 had incomplete sm_120 kernels
**Solution:** PyTorch nightly with CUDA 12.8 has full RTX 5090 support
**Result:** Everything works perfectly on RTX 5090! üéâ

The RTX 5090 is fully functional for DeepSeek-OCR inference using the Transformers backend.
